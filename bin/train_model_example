#!/usr/bin/env python

import argparse
import json
import logging
import os

import numpy as np
import torch

from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler


logging.basicConfig(level=logging.INFO)

from basismixer.predictive_models import (construct_model,
                                          RecurrentModel,
                                          SupervisedTrainer,
                                          MSELoss)
from basismixer.utils import load_pyc_bz, save_pyc_bz

LOGGER = logging.getLogger(__name__)

CONFIG = [
    dict(basis_functions='from_dataset',
         # basis_functions=['polynomial_pitch_basis',
         #                   'loudness_direction_basis',
         #                   'tempo_direction_basis',
         #                   'articulation_basis',
         #                   'duration_basis',
         #                   'fermata_basis',
         #                   'metrical_basis'],
         parameter_names=['velocity_trend', 'timing', 'log_articulation'],
         constructor=['basismixer.predictive_models', 'FeedForwardModel'],
         args=dict(
             hidden_size=128,
             input_type='notewise'
         ),
         train_args=dict(
             optimizer=dict(constructor='Adam',
                            lr=1e-4),
             epochs=3,
             save_freq=1,
             early_stopping=100,
             batch_size=10,
         )
         ),
    dict(basis_functions='from_dataset',
         # basis_functions=['polynomial_pitch_basis',
         #                  'loudness_direction_basis',
         #                  'tempo_direction_basis',
         #                  'articulation_basis',
         #                  'duration_basis',
         #                  'fermata_basis',
         #                  'metrical_basis'],
         parameter_names=['velocity_trend', 'beat_period_standardized'],
         constructor=['basismixer.predictive_models', 'RecurrentModel'],
         args=dict(
             recurrent_size=128,
             n_layers=1,
             hidden_size=64,
             input_type='onsetwise'),
         train_args=dict(
             optimizer=dict(constructor='Adam',
                            lr=1e-4),
             epochs=3,
             save_freq=1,
             early_stopping=100,
             batch_size=10,


         )
         )
]


# def main():
if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Train a Model given a dataset")
    parser.add_argument("dataset", help="Dataset(s) for training the model",
                        nargs='+')
    parser.add_argument("--model-config", help="Model Configuration",
                        default=CONFIG)
    parser.add_argument("--out-dir", help="Output Directory",
                        default='/tmp')
    args = parser.parse_args()

    # Load model architecture
    if not isinstance(args.model_config, list):
        model_config = json.load(open(args.model_config))
    else:
        model_config = args.model_config

    if not os.path.exists(args.out_dir):
        os.mkdir(args.out_dir)

    json.dump(model_config, open(os.path.join(args.out_dir, 'model_config.json'), 'w'),
              indent=4)

    if len(args.dataset) == 1:
        args.dataset *= len(model_config)
    else:
        if len(args.dataset) != len(model_config):
            raise ValueError(
                'The number of datasets should be the same '
                'as the number of models to train. '
                'Given {0} datasets and {1} models.'.format(
                    len(args.dataset), len(model_config)))

    rng = np.random.RandomState(1984)

    datasets = []
    models = []
    target_idxs = []
    input_idxs = []
    valid_size = 0.20
    for fn, config in zip(args.dataset, model_config):

        #### Construct Dataset ####
        batch_size = config['train_args'].pop('batch_size')
        # load dataset
        data = load_pyc_bz(fn)
        dataset = data['dataset']
        in_names = data['in_names']
        out_names = data['out_names']

        dataset_idx = np.arange(len(dataset))
        rng.shuffle(dataset_idx)
        len_valid = int(np.round(len(dataset) * valid_size))
        valid_idx = dataset_idx[0:len_valid]
        train_idx = dataset_idx[len_valid:]

        train_sampler = SubsetRandomSampler(train_idx)
        valid_sampler = SubsetRandomSampler(valid_idx)
        train_dataloader = DataLoader(dataset,
                                      batch_size=batch_size,
                                      # shuffle=True,
                                      sampler=train_sampler)
        valid_dataloader = DataLoader(dataset,
                                      batch_size=batch_size,
                                      sampler=valid_sampler)

        datasets.append((dataset, train_dataloader, valid_dataloader))

        #### Construct Models ####

        if config['basis_functions'] == 'from_dataset':
            model_in_names = in_names
        elif isinstance(config['basis_functions'], (list, tuple)):
            model_in_names = []
            for bf in config['basis_functions']:
                for name in in_names:
                    if name.startswith(bf):
                        model_in_names.append(name)

        i_idxs = np.array([list(in_names).index(bf) for bf in model_in_names])
        input_idxs.append(i_idxs)

        model_out_names = []
        for pn in config['parameter_names']:
            for name in out_names:
                if name == pn:
                    model_out_names.append(name)

        t_idxs = np.array([list(out_names).index(pn) for pn in model_out_names])
        target_idxs.append(t_idxs)

        config['args']['input_names'] = model_in_names
        config['args']['input_size'] = len(model_in_names)

        config['args']['output_names'] = model_out_names
        config['args']['output_size'] = len(model_out_names)

        model = construct_model(config)
        models.append(model)

    for (model, (dataset, train_loader, val_loader),
         config, t_idxs) in zip(models, datasets,
                                model_config, target_idxs):

        for d in dataset.datasets:
            d.targets_idx = t_idxs

        t_config = config['train_args']
        o_config = t_config.pop('optimizer')
        model_name = '-'.join(model.output_names) + '-' + model.input_type
        model_out_dir = os.path.join(args.out_dir, model_name)
        if not os.path.exists(model_out_dir):
            os.mkdir(model_out_dir)
        loss = MSELoss()
        optim = getattr(torch.optim, o_config.pop('constructor'))
        optimizer = optim(model.parameters(), **o_config)
        trainer = SupervisedTrainer(model=model,
                                    train_loss=loss,
                                    optimizer=optimizer,
                                    valid_loss=loss,
                                    train_dataloader=train_loader,
                                    valid_dataloader=val_loader,
                                    out_dir=model_out_dir,
                                    **t_config)
        trainer.train()
