#!/usr/bin/env python

import argparse
import json
import logging
import os
import pickle
import bz2


import numpy as np
import torch

from torch.utils.data import DataLoader

logging.basicConfig(level=logging.INFO)

from basismixer import (BASIS_CONFIG_EXAMPLE,
                        OnsetwiseDecompositionDynamicsCodec,
                        TimeCodec,
                        PerformanceCodec,
                        make_dataset)
from basismixer.predictive_models import (RecurrentModel,
                                          SupervisedTrainer,
                                          MSELoss)


def load_pyc_bz(fn):
    return pickle.load(bz2.BZ2File(fn, 'r'))


def save_pyc_bz(d, fn):
    pickle.dump(d, bz2.BZ2File(fn, 'w'), pickle.HIGHEST_PROTOCOL)


LOGGER = logging.getLogger(__name__)

if __name__ == '__main__':

    parser = argparse.ArgumentParser(
        description="Construct a dataset from files in the specified folders")
    parser.add_argument("xmlfolder", help="Folder with MusicXML files")
    parser.add_argument("matchfolder", help="Folder with match files")
    # parser.add_argument("--onsetwise",
    #                     help=("Aggregate inputs and "
    #                           "targets for sequential models"),
    #                     action='store_true', default=False)
    # parser.add_argument("--basisconfig", type=str,
    #                     help=("JSON file specifying a set of basis functions "
    #                           "for each expressive target. If not specified "
    #                           "a default configuration will be used."),
    #                     default=BASIS_CONFIG_EXAMPLE)
    args = parser.parse_args()
    # basis_config = json.load(open(args.basisconfig))
    basis_functions = ['polynomial_pitch_basis',
                       'loudness_direction_basis',
                       'tempo_direction_basis',
                       'articulation_basis',
                       'duration_basis'
                       ]
    dyn_codec = OnsetwiseDecompositionDynamicsCodec()
    time_codec = TimeCodec(normalization='bp_standardized')
    perf_codec = PerformanceCodec(time_codec, dyn_codec)

    seq_len = 20
    batch_size = 10
    dataset_fn = '/tmp/dataset.pyc.bz'
    if not os.path.exists(dataset_fn):
        dataset, in_names, out_names = make_dataset(args.xmlfolder, args.matchfolder,
                                                    basis_functions, perf_codec, seq_len,
                                                    aggregate_onsetwise=True)
        data = dict(
            dataset=dataset,
            in_names=in_names,
            out_names=out_names)
        save_pyc_bz(data, dataset_fn)
    else:
        data = load_pyc_bz(dataset_fn)
        dataset = data['dataset']
        in_names = data['in_names']
        out_names = data['out_names']

    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    LOGGER.info('Dataset has {} instances'.format(len(dataset)))
    LOGGER.info('Inputs: {}'.format(', '.join(in_names)))
    LOGGER.info('Outputs: {}'.format(', '.join(out_names)))

    device = torch.device('cpu')
    recurrent_size = 10
    hidden_size = 7
    n_layers = 1
    model = RecurrentModel(input_size=len(in_names),
                           output_size=len(out_names),
                           recurrent_size=recurrent_size,
                           hidden_size=hidden_size,
                           n_layers=n_layers,
                           input_names=in_names,
                           output_names=out_names,
                           input_type='onsetwise')
    loss = MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    trainer = SupervisedTrainer(model=model,
                                train_loss=loss,
                                optimizer=optimizer,
                                valid_loss=loss,
                                train_dataloader=dataloader,
                                valid_dataloader=dataloader,
                                epochs=3,
                                save_freq=1)

    trainer.train()
