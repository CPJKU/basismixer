#!/usr/bin/env python

import argparse
import json
import logging
import os

import numpy as np
import torch

from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data import DataLoader, ConcatDataset

logging.basicConfig(level=logging.INFO)

from basismixer.predictive_models import (construct_model,
                                          SupervisedTrainer,
                                          MSELoss)
from basismixer.utils import load_pyc_bz, save_pyc_bz
from basismixer import make_datasets

LOGGER = logging.getLogger(__name__)

# def my_basis(part):
#     W = np.array([n.midi_pitch for n in part.notes_tied]).astype(np.float)
#     return W.reshape((-1, 1)), ['my']

CONFIG = [
    dict(onsetwise=False,
         basis_functions=['polynomial_pitch_basis',
                          'loudness_direction_basis',
                          'tempo_direction_basis',
                          'articulation_basis',
                          'duration_basis',
                          # my_basis,
                          'grace_basis',
                          'slur_basis',
                          'fermata_basis',
                          'metrical_basis'],
         parameter_names=['velocity_dev', 'timing', 'articulation_log'],
         seq_len=1,
         model=dict(constructor=['basismixer.predictive_models', 'FeedForwardModel'],
                    args=dict(hidden_size=128)),
         train_args=dict(
             optimizer=['Adam', dict(lr=1e-4)],
             epochs=3,
             save_freq=10,
             early_stopping=100,
             batch_size=1000,
         )
    ),
    dict(onsetwise=True,
         basis_functions=['polynomial_pitch_basis',
                          'loudness_direction_basis',
                          'tempo_direction_basis',
                          'articulation_basis',
                          'duration_basis',
                          'slur_basis',
                          'grace_basis',
                          'fermata_basis',
                          'metrical_basis'],
         parameter_names=['velocity_trend', 'beat_period_standardized',
                          'beat_period_mean', 'beat_period_std'],
         seq_len=100,
         model=dict(constructor=['basismixer.predictive_models', 'RecurrentModel'],
                    args=dict(recurrent_size=128,
                              n_layers=1,
                              hidden_size=64)),
         train_args=dict(
             optimizer=['Adam', dict(lr=1e-4)],
             epochs=3,
             save_freq=5,
             early_stopping=100,
             batch_size=50,
         )
    )
]

def jsonize_dict(input_dict):
    out_dict = dict()
    for k, v in input_dict.items():
        if isinstance(v, np.ndarray):
            out_dict[k] = v.tolist()
        elif isinstance(v, dict):
            out_dict[k] = jsonize_dict(v)
        else:
            out_dict[k] = v
    return out_dict

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Train a Model given a dataset")
    parser.add_argument("xmlfolder", help="Folder with MusicXML files")
    parser.add_argument("matchfolder", help="Folder with match files")
    parser.add_argument("--datasets", help=(
        'Path to pickled datasets file. If specified and the file exists, '
        'the `xmlfolder` and `matchfolder` options will be ignored, and it '
        'will be assumed that datasets in the specified file correspond to '
        'the model configuration. If specifed and the path does not exist, '
        'the datasets are computed and saved to the specified path.'))
    parser.add_argument("--quirks", action='store_true',
                        help="Use this option when training on magaloff/zeilinger")
    parser.add_argument("--pieces", help="Text file with valid pieces",
                        default=None)
    parser.add_argument("--model-config", help="Model configuration",
                        default=CONFIG)
    parser.add_argument("--out-dir", help="Output directory",
                        default='/tmp')
    args = parser.parse_args()

    # Load model architecture
    if not isinstance(args.model_config, list):
        model_config = json.load(open(args.model_config))
    else:
        model_config = args.model_config

    if not os.path.exists(args.out_dir):
        os.mkdir(args.out_dir)

    json.dump(model_config,
              open(os.path.join(args.out_dir, 'model_config.json'), 'w'),
              indent=2)

    if args.pieces is not None:
        print('valid_pieces')
        args.pieces = np.loadtxt(args.pieces, dtype=str)

    rng = np.random.RandomState(1984)

    datasets = []
    models = []
    target_idxs = []
    input_idxs = []
    valid_size = 0.20

    if args.datasets and os.path.exists(args.datasets):
        LOGGER.info('Loading data from {}'.format(args.datasets))
        datasets = load_pyc_bz(args.datasets)
    else:
        datasets = make_datasets(model_config,
                                 args.xmlfolder,
                                 args.matchfolder,
                                 pieces=args.pieces,
                                 quirks=args.quirks)
        if args.datasets:
            LOGGER.info('Saving data to {}'.format(args.datasets))
            save_pyc_bz(datasets, args.datasets)

    for (mdatasets, in_names, out_names), config in zip(datasets, model_config):
        dataset = ConcatDataset(mdatasets)
        batch_size = config['train_args'].pop('batch_size')

        #### Create train and validation data loaders #####
        dataset_idx = np.arange(len(dataset))
        rng.shuffle(dataset_idx)
        len_valid = int(np.round(len(dataset) * valid_size))
        valid_idx = dataset_idx[0:len_valid]
        train_idx = dataset_idx[len_valid:]

        train_sampler = SubsetRandomSampler(train_idx)
        valid_sampler = SubsetRandomSampler(valid_idx)
        train_loader = DataLoader(dataset,
                                  batch_size=batch_size,
                                  sampler=train_sampler)
        valid_loader = DataLoader(dataset,
                                  batch_size=batch_size,
                                  sampler=valid_sampler)

        #### Construct Models ####

        model_cfg = config['model'].copy()
        model_cfg['args']['input_names'] = in_names
        model_cfg['args']['input_size'] = len(in_names)
        model_cfg['args']['output_names'] = out_names
        model_cfg['args']['output_size'] = len(out_names)
        model_name = ('-'.join(out_names) +
                      '-' + ('onsetwise' if config['onsetwise'] else 'notewise'))
        model_out_dir = os.path.join(args.out_dir, model_name)
        if not os.path.exists(model_out_dir):
            os.mkdir(model_out_dir)
        # save model config for later saving model
        json.dump(jsonize_dict(model_cfg),
                  open(os.path.join(model_out_dir, 'config.json'), 'w'),
                  indent=2)
        model = construct_model(model_cfg)

        loss = MSELoss()

        ### Construct the optimizer ####
        optim_name, optim_args = config['train_args']['optimizer']
        optim = getattr(torch.optim, optim_name)
        config['train_args']['optimizer'] = optim(model.parameters(), **optim_args)

        trainer = SupervisedTrainer(model=model,
                                    train_loss=loss,
                                    valid_loss=loss,
                                    train_dataloader=train_loader,
                                    valid_dataloader=valid_loader,
                                    out_dir=model_out_dir,
                                    **config['train_args'])

        trainer.train()

