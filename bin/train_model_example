#!/usr/bin/env python

import argparse
import json
import logging
import os

import numpy as np
import torch

from torch.utils.data import DataLoader
from torch.utils.data.sampler import SubsetRandomSampler

logging.basicConfig(level=logging.INFO)

from basismixer.predictive_models import (construct_model,
                                          SupervisedTrainer,
                                          MSELoss)
from basismixer.utils import load_pyc_bz, save_pyc_bz
from basismixer import make_datasets

LOGGER = logging.getLogger(__name__)

# def my_basis(part):
#     W = np.array([n.midi_pitch for n in part.notes_tied]).astype(np.float)
#     return W.reshape((-1, 1)), ['my']

CONFIG = [
    dict(onsetwise=False,
         basis_functions=['polynomial_pitch_basis',
                          'loudness_direction_basis',
                          'tempo_direction_basis',
                          'articulation_basis',
                          'duration_basis',
                          # my_basis,
                          'slur_basis',
                          'fermata_basis',
                          'metrical_basis'],
         parameter_names=['velocity_trend', 'timing', 'articulation_log'],
         seq_len=1,
         model=dict(constructor=['basismixer.predictive_models', 'FeedForwardModel'],
                    args=dict(hidden_size=128)),
         train_args=dict(
             optimizer=['Adam', dict(lr=1e-4)],
             epochs=3,
             save_freq=1,
             early_stopping=100,
             batch_size=10,
         )
    ),
    dict(onsetwise=True,
         basis_functions=['polynomial_pitch_basis',
                          'loudness_direction_basis',
                          'tempo_direction_basis',
                          'articulation_basis',
                          'duration_basis',
                          'slur_basis',
                          'fermata_basis',
                          'metrical_basis'],
         parameter_names=['velocity_trend', 'beat_period_standardized', 'beat_period_mean', 'beat_period_std'],
         seq_len=10,
         model=dict(constructor=['basismixer.predictive_models', 'RecurrentModel'],
                    args=dict(recurrent_size=128,
                              n_layers=1,
                              hidden_size=64)),
         train_args=dict(
             optimizer=['Adam', dict(lr=1e-4)],
             epochs=3,
             save_freq=1,
             early_stopping=100,
             batch_size=10,
         )
    )
]


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description="Train a Model given a dataset")
    parser.add_argument("xmlfolder", help="Folder with MusicXML files")
    parser.add_argument("matchfolder", help="Folder with match files")
    # parser.add_argument("dataset", help="Dataset(s) for training the model",
    #                     nargs='+')
    parser.add_argument("--model-config", help="Model configuration",
                        default=CONFIG)
    parser.add_argument("--out-dir", help="Output directory",
                        default='/tmp')
    args = parser.parse_args()

    # Load model architecture
    if not isinstance(args.model_config, list):
        model_config = json.load(open(args.model_config))
    else:
        model_config = args.model_config

    if not os.path.exists(args.out_dir):
        os.mkdir(args.out_dir)

    json.dump(model_config, open(os.path.join(args.out_dir, 'model_config.json'), 'w'),
              indent=2)

    rng = np.random.RandomState(1984)

    datasets = []
    models = []
    target_idxs = []
    input_idxs = []
    valid_size = 0.20

    datasets = make_datasets(model_config, args.xmlfolder, args.matchfolder)
    
    for (dataset, in_names, out_names), config in zip(datasets, model_config):

        batch_size = config['train_args'].pop('batch_size')

        #### Create train and validation data loaders #####
        dataset_idx = np.arange(len(dataset))
        rng.shuffle(dataset_idx)
        len_valid = int(np.round(len(dataset) * valid_size))
        valid_idx = dataset_idx[0:len_valid]
        train_idx = dataset_idx[len_valid:]

        train_sampler = SubsetRandomSampler(train_idx)
        valid_sampler = SubsetRandomSampler(valid_idx)
        train_loader = DataLoader(dataset,
                                  batch_size=batch_size,
                                  # shuffle=True,
                                  sampler=train_sampler)
        valid_loader = DataLoader(dataset,
                                  batch_size=batch_size,
                                  sampler=valid_sampler)

        #### Construct Models ####
        model_cfg = config['model'].copy()
        model_cfg['args']['input_names'] = in_names
        model_cfg['args']['input_size'] = len(in_names)
        model_cfg['args']['output_names'] = out_names
        model_cfg['args']['output_size'] = len(out_names)
        model = construct_model(model_cfg)

        model_name = ('-'.join(model.output_names) +
                      '-' + 'onsetwise' if config['onsetwise'] else 'notewise')
        model_out_dir = os.path.join(args.out_dir, model_name)

        if not os.path.exists(model_out_dir):
            os.mkdir(model_out_dir)

        loss = MSELoss()

        ### Construct the optimizer ####
        optim_name, optim_args = config['train_args']['optimizer']
        optim = getattr(torch.optim, optim_name)
        config['train_args']['optimizer'] = optim(model.parameters(), **optim_args)

        trainer = SupervisedTrainer(model=model,
                                    train_loss=loss,
                                    # optimizer=optimizer,
                                    valid_loss=loss,
                                    train_dataloader=train_loader,
                                    valid_dataloader=valid_loader,
                                    out_dir=model_out_dir,
                                    **config['train_args'])
        trainer.train()
